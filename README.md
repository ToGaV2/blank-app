# ðŸŽˆ Simple Groq Chat LLM Implementation with LLaMa 3.1

Ever wanted to build a super fast chat engine inside a  python streamlit? 

Here is a good example. You'll just need a login at Groq, where you can get an API Key. Load the API key into the toml as instructed in step 2.


[![Open in Streamlit](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://blank-app-template.streamlit.app/)

### How to run it on your own machine

1. Install the requirements

   ```
   $ pip install -r requirements.txt
   ```

2. Get a Groq API key
   
   ```
   https://console.groq.com/keys

   install this in CWD/.streamlit/secrets.toml as "GROQ_API_KEY"
   ```

4. Run the app

   ```
   $ streamlit run streamlit_app.py
   ```
